# nlp-getting-started

Kaggle competition: [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started)

ğŸ¯ **Goal**: Classify tweets as real disasters (1) or not (0).

ğŸ“Š **Public Score**: `0.76`

---

## ğŸ§° Approach

- Basic text feature engineering (length, caps, punctuation counts)
- TF-IDF vectorization (5000 features, n-grams 1-2)
- CatBoost classifier with keyword as categorical feature
- Trained offline in Kaggle Notebook (no internet)

---

## ğŸ“ Files

- `notebook.ipynb` â€” main training and prediction notebook
- `submission.csv` â€” submission file (F1 ~0.76)

---

## ğŸš€ How to Reproduce

1. Run the notebook in Kaggle.
2. Submit `submission.csv` to the competition.

---

## ğŸ“ˆ Next Steps

- Add BERT embeddings â†’ target: 0.83+
- Hyperparameter tuning with Optuna
- Model ensembling

---
